{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[知识篇——监督学习算法优缺点及应用场景概览](https://www.jianshu.com/p/20bffd53101a)\n",
    "[Naive Bayes and Text Classification](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
    "[Vectorization, Multinomial Naive Bayes Classifier and Evaluation](https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 监督学习\n",
    "\n",
    "[scikit-learn监督学习](http://scikit-learn.org/stable/supervised_learning.html) 中选择以下模型:\n",
    "- 高斯朴素贝叶斯 (GaussianNB)\n",
    "- 决策树 (DecisionTree)\n",
    "- 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K近邻 (K Nearest Neighbors)\n",
    "- 随机梯度下降分类器 (SGDC)\n",
    "- 支持向量机 (SVM)\n",
    "- Logistic回归（LogisticRegression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯朴素贝叶斯(GaussianNB)\n",
    "\n",
    "高斯朴素贝叶斯假设特征的可能性(即概率)为高斯分布，在sklearn库中对应的算法为sklearn.naive_bayes.GaussianNB，该算法涉及到的参数有\n",
    "- priors:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。\n",
    "\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "该模型的优势：\n",
    "- 能够处理大量特征，即使存在不相关的特征也有很好的效果而且不容易受到该特征的影响。\n",
    "- 可以有效缓解维度灾难带来的问题，这是由于分类条件的解耦意味着可以独立地把每个特征视为一维分布来估计。\n",
    "- 相对比较简单，完全可以直接使用，很少需要调整参数，通常除非分布数据已知的情况下需要调整。\n",
    "- 很少会出现拟合。\n",
    "- 相对于它处理的数据量来说，训练和预测的速度很快。\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "该模型的缺点：\n",
    "- 该模型是很好的分类器，却不是很好的估计器，所以不能太过于重视从`predict_proba`输出的概率。\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答：\n",
    "\n",
    "**参考资料**\n",
    "- [sklearn中文文档](https://sklearn.apachecn.org/docs/0.21.3/10.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树 (DecisionTree)\n",
    "\n",
    "决策树创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。\n",
    "决策树的优势：\n",
    "- 树的结构可以可视化呈现，便于理解和解释。\n",
    "- 训练需要的数据少。其他机器学习模型通常需要数据规范化，比如构建虚拟变量和移除缺失值,不过请注意，这种模型不支持缺失值。\n",
    "- 由于训练决策树的数据点的数量导致了决策树的使用开销呈指数分布(训练树模型的时间复杂度是参与训练数据点的对数值)。\n",
    "- 能够处理数值型数据和分类数据。其他的技术通常只能用来专门分析某一种变量类型的数据集。\n",
    "- 能够处理多路输出的问题。\n",
    "- 使用白盒模型。如果某种给定的情况在该模型中是可以观察的，那么就可以轻易的通过布尔逻辑来解释这种情况。相比之下，在黑盒模型中的结果就是很难说明清楚地。\n",
    "- 可以通过数值统计测试来验证该模型。这对事解释验证该模型的可靠性成为可能。\n",
    "- 即使该模型假设的结果与真实模型所提供的数据有些违反，其表现依旧良好。\n",
    "\n",
    "决策树的缺点：\n",
    "- 决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现 该问题最为有效地方法。\n",
    "- 决策树可能是不稳定的，因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解\n",
    "- 在多方面性能最优和简单化概念的要求下，学习一棵最优决策树通常是一个NP难问题。因此，实际的决策树学习算法是基于启发式算法，例如在每个节点进 行局部最优决策的贪心算法。这样的算法不能保证返回全局最优决策树。这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。\n",
    "- 有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。\n",
    "- 如果某些类在问题中占主导地位会使得创建的决策树有偏差。因此，我们建议在拟合前先对数据集进行平衡。\n",
    "\n",
    "### 决策树分类\n",
    "\n",
    "决策树分类能够在数据集上执行多分类,在sklearn中对应的算法模型为sklearn.tree.DecisionTreeClassifier，常见的超参数有：\n",
    "- max_depth：树中的最大层级数量。\n",
    "- min_samples_leaf：叶子允许的最低样本数量。\n",
    "- min_samples_split：拆分内部节点所需的最低样本数量。\n",
    "- max_features：寻找最佳拆分方法时要考虑的特征数量。\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "该模型的优点：\n",
    "\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "该模型的缺点：\n",
    "\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近邻\n",
    "\n",
    "### K近邻 (K Nearest Neighbors)\n",
    "基于每个查询点的K个最近邻实现，其中K是用户指定的整数值。在sklearn库中对应的算法为sklearn.neighbors.KNeighborsClassifier该算法涉及到的参数有：\n",
    "- n_neighbor\n",
    "- weights:对邻居进行加权，使得更近邻更有利于拟合,\n",
    "    - `uniform`为每个近邻分配统一的权重，默认值。\n",
    "    - `distance`分配权重与查询点的距离成反比。\n",
    "    - `[callable]`用户可以自定义一个距离函数用来计算权重。\n",
    "- algorithm：\n",
    "    \n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "该模型的缺点：\n",
    "- 值的最佳选择是高度依赖数据的：通常较大的 k 是会抑制噪声的影响，但是使得分类界限不明显。\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答：\n",
    "\n",
    "**参考资料**\n",
    "- [sklearn中文文档:最邻近](https://sklearn.apachecn.org/docs/0.21.3/7.html)\n",
    "\n",
    "### 基于半径的近邻分类\n",
    "\n",
    "**基于半径的近邻分类**\n",
    "\n",
    "基于每个查询点的固定半径r内的邻居数量实现，其中r是用户指定的浮点数值。在sklearn中对应的算法为sklearn.neighbors.RadiusNeighborsClassifier涉及到的参数为：\n",
    "- radius:计算邻居数量的半径，默认值为1。\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "该模型的优势：\n",
    "- 如果数据是不均匀采样，该算法比K近邻算法表现更好一点儿\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "该模型的缺点：\n",
    "- 对于高维参数空间，维度灾难会使得算法不再那么有效。\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降(SGD)\n",
    "随机梯度下降(SGD)是一种简单但又非常高效的方法，主要用于凸损失函数下线性分类器的判别式学习，例如(线性)支持向量机和Logistic回归。SGD已成功应用于在文本分类和自然语言处理中经常遇到的大规模和稀疏的机器学习问题。SGD的优缺点：\n",
    "\n",
    "SGD的优点：\n",
    "- 高效且易于实现\n",
    "\n",
    "SGD的缺点：\n",
    "- 需要一些超参数，如regularization(正则化)参数和number of iterations(迭代次数)。\n",
    "- 对特征缩放敏感。\n",
    "\n",
    "### 随机梯度下降分类器 (SGDC)\n",
    "\n",
    "SGDC实现了一个简单的随机梯度下降学习例程, 支持分类问题不同的损失函数和正则化方法。对于稀疏数据，SGDClassifier分类器可以轻易的处理超过10^5的训练样本和超过10^5的特征。在sklearn中对应的算法为sklearn.linear_model.SGDClassifier，该算法涉及到的参数有：\n",
    "- loss:损失函数\n",
    "    - hinge:(软-间隔)线性支持向量机,hinge为默认值\n",
    "    - log:logistic回归\n",
    "    - modified_huber:平滑的 hinge 损失\n",
    "    - squared_hinge\n",
    "    - perceptron\n",
    "    a regression loss: \n",
    "    - squared_loss\n",
    "    - huber\n",
    "    - epsilon_insensitive\n",
    "    - squared_epsilon_insensitive\n",
    "- penalty:惩罚方法\n",
    "    - none\n",
    "    - l2,默认值为l2。\n",
    "    - l1:L1惩罚会导致稀疏解，使得大多数系数为零。\n",
    "    - elasticnet:弹性网(L2型和L1型的凸组合:(1 - l1_ratio) * L2 + l1_ratio * L1),解决了在特征高相关时L1惩罚的一些不足。参数l1_ratio 控制了L1和L2惩罚的凸组合。\n",
    "\n",
    "\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "该模型的优势：\n",
    "- \n",
    "- \n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "该模型的缺点：\n",
    "- \n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答：\n",
    "\n",
    "**参考资料**\n",
    "- [sklearn中文文档:SGDC](https://sklearn.apachecn.org/docs/0.21.3/6.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机 (SVM)\n",
    "\n",
    "支持向量机(SVMs)可用于分类、回归和异常检测监督学习算法：\n",
    "\n",
    "支持向量机的优势在于:\n",
    "- 在高维空间中非常高效.\n",
    "- 即使在数据维度比样本数量大的情况下仍然有效.\n",
    "- 在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的.\n",
    "- 通用性: 不同的核函数核函数与特定的决策函数一一对应.常见的 kernel 已经提供,也可以指定定制的内核.\n",
    "\n",
    "支持向量机的缺点包括:\n",
    "- 如果特征数量比样本数量大得多,在选择核函数 核函数 时要避免过拟合, 而且正则化项是非常重要的.\n",
    "- 支持向量机不直接提供概率估计,这些都是使用昂贵的五次交叉验算计算的.\n",
    "\n",
    "### SVC\n",
    "\n",
    "sklearn中对应的算法为sklearn.svm.SVC，其中最常见的超参数包括：\n",
    "- C：C 参数。\n",
    "- kernel：内核。最常见的内核为 'linear'、'poly' 和 'rbf'。\n",
    "- degree：如果内核是多项式，则此参数为内核中的最大单项式次数。\n",
    "- gamma：如果内核是径向基函数，则此参数为 γ 参数。\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答：\n",
    "\n",
    "**参考资料**\n",
    "- [sklearn中文文档:支持向量机](https://sklearn.apachecn.org/docs/0.21.3/5.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic回归（LogisticRegression）\n",
    "**模型名称**\n",
    "\n",
    "回答：\n",
    "\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 模型2\n",
    "\n",
    "**模型名称**\n",
    "\n",
    "回答：\n",
    "\n",
    "\n",
    "**描述一个该模型在真实世界的一个应用场景。（你需要为此做点研究，并给出你的引用出处）**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的优势是什么？他什么情况下表现最好？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**这个模型的缺点是什么？什么条件下它表现很差？**\n",
    "\n",
    "回答：\n",
    "\n",
    "**根据我们当前数据集的特点，为什么这个模型适合这个问题。**\n",
    "\n",
    "回答："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
